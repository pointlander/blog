---
title: "Everett Activation Function"
date: 2020-05-11T20:47:37-06:00
draft: false
---

The [Everett interpretation](https://en.wikipedia.org/wiki/Many-worlds_interpretation) of quantum mechanics holds that there are infinitely many universes. The universe branches with each measurement. The act of measurement is similar to the activation functions used in neural networks. The output of a neuron could be fed into two activation functions. The [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and its complement could be used: max(0, x) and min(0, x). When added together in a neuron: w1 * max(0, x) + w2 * min(0, x) = x (for w1 = w2 = 1) which is the identity function, so gradient descent decides if the Everett activation function is an identity function.

In [Everything as a Quantum Neural Network](/posts/everything-as-a-quantum-neural-network) I state that everything could be viewed as a quantum neural network. I think the universe does branch, but the universes also recombine forming a neural network with Everett activation functions. Perhaps the sum of all of the universes is zero, so the multiverse is a quantum neural network that must output zero. The universe began as nothing, and then a measurement happens, and then the universe split into two parts that sum to zero.

Shout out to [Devs](https://en.wikipedia.org/wiki/Devs)
